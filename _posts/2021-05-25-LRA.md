---
layout: post
title: "[논문리뷰] Long Range Arena: A Benchmark for Efficient Transformers"
use_math: true
tags:
- 딥러닝
- 논문리뷰
date:   2021-05-25 12:00:00 
lastmod : 2021-05-25 12:00:00
sitemap :
  changefreq : daily
  priority : 1.0
---

## 요약
* <span class="highlight-yellow">이 논문은 Long Range Arena(LRA)라는 이름의 Benchmark Dataset에 대해</span>\\
  <span class="highlight-yellow">설명하는 논문으로, 트랜스포머 모델이 긴 시퀀스에 대해 얼마나 효과적인지 평가하기 위한 데이터셋이며, 6가지 서브 태스크로 구성되어있다.</span>
* 트랜스포머 모델의 self-attention은 행렬곱 연산으로 인해 token의 길이가 길어지면 complexity가 quadratic하게 증가하고,
  거리가 먼 token간의 연관성을 학습하는데 어려움을 겪는다.
* 긴 시퀀스 문제를 트랜스포머로 해결하고자 할 때, 어떤 요소를 통해 평가할지 고민하고, 그에 맞는 여러 서브 데이터셋을 제시하였다.
* 각 태스크의 시퀀스의 길이는 1K에서 16K까지 다양하며, text, natural, synthetic images, mathematical expressions 등 여러 modality의 데이터로 구성되었다.
* 각 데이터셋에 대하여 지금까지 나온 유명한 트랜스포머 아키텍쳐 몇 개를 선정하여  baseline을 제시하였다.

## Desiderata: 이 데이터셋이 추구하는 것
1. Generality: 여러 태스크에 적용이 가능해야 한다. 가령 모든 트랜스포머 모델이 autoregressive decoding이 가능한 것은 아니기 때문에 encoding으로만 할 수 있는 태스크로 구성한다.
2. Simplicity: 태스크가 간단해야 한다. 특정한 augmentation이 필요하거나, 외부 데이터셋에서 pretraining이 필요한 경우는 제외한다.
3. Challenging: 충분히 어려워서 연구할만한 가치를 지녀야한다.
4. Long inputs: Input sequence length가 충분히 길어야한다.
5. Probing diverse aspects: 각 서브 태스크는 다른 요소를 평가할 수 있어야한다. 예를 들어, relation, hierarchical/spatial structures, generalization capability 등의 요소가 내재되어야한다.
6. Non-resource intensive and accessible: 컴퓨팅 리소스에 구애받지 않도록 lightweight한 규모로 구성한다.

## 태스크
1. LONG LISTOPS\\
  긴 맥락에서 hierarchical structure을 모델링할 수 있는지 평가하는 태스크다.\\
  데이터셋은 Max, Mean, Median, 그리고 Sum_mod 연산이 괄호와 함께 포함되어있으며, 0에서 9까지 10개의 클래스로 분류하는 문제다.\\
  구체적인 Input과 Output의 예시는 다음과 같다.\\
  <span style="background-color: #DBDBDB">Input: [ MAX 4 3 [ MIN 2 3 ] 1 0 [ MEDIAN 1 5 8 9, 2 ] ]</span>\\
  <span style="background-color: #DBDBDB">Output: 5 </span>\\
  문제는 간단하다. Input의 연산을 풀어서 답을 찾으면 되고,\\
  답도 0에서 9까지만 있어 분류 문제다.\\
  사람이 위 문제를 풀기 위해선 다음의 사고 과정을 거친다.\\
  먼저 우선적으로 풀어야하는 괄호를 찾아 Min값 2(2와3중에 2)과 Median값 5(1,2,5,8,9 중에 5, 쉼표(,)는 노이즈다.)를 구하고
  해당 괄호 부분을 2와 5로 대체하여 준다(\[MAX 4 3 2 1 0 5\]).
  그 다음 Max값 5(4,3,2,1,0,5 중에 5)를 얻는다.\\
  이러한 기다란 수학 문제로 구성된 논리적인 구조를 트랜스포머가 풀 수 있는지 확인할 수 있는 태스크다.\\
  단, Input은 텍스트 데이터로 입력받으며 길이가 최대 2,000까지 된다.
  
2. BYTE-LEVEL TEXT CLASSIFICATION\\
  Byte/character-level에서의 language modeling(char LM)이다.\\
  단어 단위로 토큰을 만드는 것이 아니라\\
  알파벳 하나 단위로 토큰을 만드는 것이다.\\
  예를 들면, 영어 사용자는 appl의 다음 알파벳은 e가 나올 것 같은 맥락 정보를 생각할 수 있다.\\
  이렇게 글을 알파벳 단위로 작게 나누더라도 트랜스포머가 글을 이해할 수 있는지 확인하는 태스크다.\\
  구체적으로 IMDb reviews 데이터셋의 문장을\\
  알파벳 단위로 임베딩한 것을 input으로,\\
  output은 binary(good/bad)값으로 하는 이진 분류 문제를 푼다.\\
  알파벳 단위로 나누게 되면 길이가 최대 4,000까지 길어진다.

3. BYTE-LEVEL DOCUMENT RETRIEVAL\\
  두 문서가 얼마나 유사한지 모델링하는 문제로,\\
  문서를 byte/character-level로 임베딩하여 긴 시퀀스를 만들고,\\
  압축된 representation간의 similarity를 비교하여 매칭하는 테스크다.\\
  데이터는 input에 두 개의 논문이 들어가고, output은 두 논문간 인용이 되어있다면 1, 관련이 없다면 0으로 이진 분류 문제다.\\
  문서의 길이는 4,000이고 두 개를 합치기 때문에 8,000까지 늘어나게 된다.

4. IMAGE CLASSIFICATION ON SEQUENCES OF PIXELS\\
  2차원의 이미지를 1차원으로 만들어 입력하는 문제다.\\
  이미지가 1차원의 긴 픽셀로 만들어졌을 때, 픽셀간 spatial relation을 트랜스포머가 학습할 수 있는지 여부를 확인한다.\\
  데이터셋은 CIFAR-10을 이용하며 gray-scale로 변환하여 사용하는 이미지 분류 문제이다.
  
5. PATHFINDER (LONG-RANGE SPATIAL DEPENDENCY)
  [![pathfinder](/../public/images/posts/LRA_pathfinder.png)](/../public/images/posts/LRA_pathfinder.png)
  2차원의 공간에서 두 점이 선으로 연결되어있는지 여부를 판단하는 문제다.\\
  위 그림에서 보듯이, 두 점이 연결되면 1, 연결되지 않으면 0으로 분류한다.\\
  완전히 채워진 선이 아니라 점선으로 구성하여 노이즈가 들어있다.\\
  이 태스크도 4번과 마찬가지로 2차원 이미지를 1차원으로 바꾸어 풀며,\\
  길이는 $32 \times 32 = 1024$이다.

6. PATHFINDER-X (LONG-RANGE SPATIAL DEPENDENCIES WITH EXTREME LENGTHS)\\
  위 5번 문제와 동일한데 차원을 훨씬 크게 늘렸다.\\
  길이는 $128 \times 128 = 16384$이다.\\
  이 문제는 5번 문제는 해결한 것과 연관지어 살펴보기 적절하다.\\
  한 알고리즘이 같은 문제를 풀지만 짧은 길이는 풀고, 긴 길이는 못 푼다면\\
  길이가 미치는 영향에 대해서 생각해볼 수 있다.

## Baseline
> [![baseline](/../public/images/posts/LRA_baseline.PNG)](/../public/images/posts/LRA_baseline.PNG)
  Performance Baseline
  
* 6가지 태스크에 대하여 기본 트랜스포머와 variants 10개로 실험했고,
  다만 각 모델의 저자가 직접 실험한 것이 아니기 때문에 대략적인 결과라는 것을 강조했다.\\
  6가지 태스크 모두 분류 문제이기 때문에 metric은 accuracy이며,\\
  각 태스크에 대해 가장 높은 값은 **볼드체**로, 두 번째로 높은 값은 <ins>밑줄</ins>로 표시했다.\\
  6번 문제인 Pathfinder-X에 대해선 모든 트랜스포머가 아직 풀지 못했다.
  
> [![cost](/../public/images/posts/LRA_baseline2.PNG)](/../public/images/posts/LRA_baseline2.PNG)
  Cost Baseline
  
* 학습 속도와 메모리도 비교해보았다.

---

## Reference
1. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, Donald Metzler\\
   Long Range Arena: A Benchmark for Efficient Transformers\\
   [https://arxiv.org/abs/2011.04006](https://arxiv.org/abs/2011.04006)

---
<script src="https://utteranc.es/client.js"
        repo="gogoymh/gogoymh.github.io"
        issue-term="pathname"
        label="Comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
    </script>
